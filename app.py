import streamlit as st
import openai
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate
from langchain_core.prompts import AIMessagePromptTemplate

# Configuration de l'application
st.set_page_config(
    page_title="AI Tutor Pro",
    page_icon="ğŸ§ ",
    layout="centered",
    initial_sidebar_state="expanded"
)

# RÃ©cupÃ©ration de la clÃ© API depuis secrets.toml (Streamlit Cloud)
openai.api_key = st.secrets["OPENAI_API_KEY"]

# Titre stylisÃ©
st.markdown("""
    <h1 style='text-align: center; color: #2B7A78; 
    border-bottom: 3px solid #17252A; padding-bottom: 10px;'>
    ğŸ“ AI Tutor Pro - Your Smart Learning Companion
    </h1>
""", unsafe_allow_html=True)

# Configuration du modÃ¨le dans la sidebar
with st.sidebar:
    st.header("âš™ï¸ ParamÃ¨tres du modÃ¨le")
    model_name = st.selectbox("ModÃ¨le LLM", ["gpt-3.5-turbo", "gpt-4"], index=0)
    temperature = st.slider("CrÃ©ativitÃ© (temperature)", 0.0, 1.0, 0.7)
    max_length = st.slider("Longueur maximale", 100, 2000, 1000)

    st.header("ğŸ’¬ Historique de chat")
    if st.button("Effacer l'historique", use_container_width=True):
        st.session_state.clear()
        st.rerun()

    st.download_button(
        label="Exporter l'historique",
        data="\n".join([f"{msg['role']}: {msg['content']}" for msg in st.session_state.get('history', [])]),
        file_name="chat_history.txt",
        mime="text/plain",
        use_container_width=True
    )

# Gestion de l'historique
if "history" not in st.session_state:
    st.session_state.history = []
    st.session_state.history.append({
        "role": "system",
        "content": "Vous Ãªtes un tuteur IA expert pour collÃ©giens. Expliquez de maniÃ¨re claire et concise, avec des exemples concrets."
    })

# Interface de chat principale
for message in st.session_state.history:
    if message["role"] not in ["system", "assistant", "user"]:
        continue
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Gestion de l'entrÃ©e utilisateur
if prompt := st.chat_input("Posez votre question..."):
    # Ajout du message utilisateur
    st.session_state.history.append({"role": "user", "content": prompt})

    with st.chat_message("user"):
        st.markdown(prompt)

    # GÃ©nÃ©ration de la rÃ©ponse via OpenAI API
    with st.chat_message("assistant"):
        response_placeholder = st.empty()
        full_response = ""

        try:
            response = openai.ChatCompletion.create(
                model=model_name,
                messages=[{"role": msg["role"], "content": msg["content"]} for msg in st.session_state.history],
                temperature=temperature,
                max_tokens=max_length
            )

            full_response = response["choices"][0]["message"]["content"]
            response_placeholder.markdown(full_response)

        except Exception as e:
            full_response = f"âŒ Erreur: {str(e)}"
            response_placeholder.error(full_response)

        # Mise Ã  jour de l'historique
        st.session_state.history.append({"role": "assistant", "content": full_response})

# Section d'information
with st.expander("â„¹ï¸ Ã€ propos de cette application"):
    st.markdown("""
    **FonctionnalitÃ©s clÃ©s :**
    - ğŸ¨ Interface utilisateur moderne et professionnelle
    - ğŸ”„ Historique de conversation persistante
    - âš™ï¸ ParamÃ¨tres ajustables en temps rÃ©el
    - ğŸ“¤ Exportation des conversations
    - ğŸš€ GÃ©nÃ©ration de rÃ©ponse avec OpenAI API
    - ğŸ› ï¸ Gestion des erreurs robuste
    - ğŸ“± Design responsive
        
    **Technologies utilisÃ©es :**
    - [OpenAI](https://platform.openai.com/) - API LLM
    - [LangChain](https://www.langchain.com/) - Orchestration LLM
    - [Streamlit](https://streamlit.io/) - Interface utilisateur
    """)
